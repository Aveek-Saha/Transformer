# Transformer
 
A TensorFlow 2.x implementation of the Transformer from `Attention Is All You Need` (Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin, arxiv, 2017). 
 
This is my attempt at trying to understand and recreate the transformer from the research paper `Attention is all you need`. This is just for my own understanding of the subject and is by no means perfect. In order to understand and implement the transformer I've taken the help of various tutorials and code guides, which I'll be linking in the resources section.
 
## How to run
 
## Resources
 
- https://mlexplained.com/2017/12/29/attention-is-all-you-need-explained/
- http://jalammar.github.io/illustrated-transformer/
- https://github.com/Kyubyong/transformer/
- https://github.com/tensorflow/tensor2tensor
- https://www.tensorflow.org/tutorials/text/transformer
- The original paper: https://arxiv.org/pdf/1706.03762.pdf
 